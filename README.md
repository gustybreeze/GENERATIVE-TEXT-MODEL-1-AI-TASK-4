# GENERATIVE-TEXT-MODEL-1-AI-TASK-4

COMPANY: CODTECH IT SOLUTIONS PVT.LTD

NAME: SAMEER KUMAR MISHRA

INTERN ID: CT04DZ379

DOMAIN: PYTHON PROGRAMMING

DURATION: 4 WEEKS

MENTOR: NEELA SANTHOSH KUMAR


**Overview**
The Generative Text Model is an exciting application of Natural Language Processing (NLP) where a machine is trained to generate human-like text based on a given prompt. In this task, we implemented a basic generative model using techniques like Recurrent Neural Networks (RNN) or Long Short-Term Memory (LSTM) to produce coherent paragraphs mimicking natural language flow.

This project showcases the use of AI in creative writing, text prediction, chatbots, and language modeling.


**Objective**
- Understand sequence modeling and its role in text generation.

- Implement and train a text generation model using LSTM or other RNN architectures.

- Input a seed/prompt, and output an auto-generated paragraph or sentence.

- Learn how to tokenize, vectorize, and generate text iteratively using trained weights.


**Tools and Technologies**
- Python 3.x

- TensorFlow / Keras

- NumPy

- Matplotlib (for optional training visualization)

- NLP libraries (optional) – NLTK / spaCy for preprocessing


**Folder Structure**
bash
Copy
Edit
task_4_generative_text_model/
│
├── text_generator.py          # Main Python script
├── input_corpus.txt           # Text data used for training
├── output.txt                 # Sample generated output
├── README.md                  # Project explanation
├── model_weights.h5           # Trained model weights (optional)
└── requirements.txt           # Python dependencies


**How It Works**

*Data Collection:*

A text file (input_corpus.txt) is used as training data. This could be poems, stories, quotes, or any text corpus.

*Preprocessing:*

- Convert text to lowercase, remove special characters.

- Tokenize text into characters or words.

- Create input-output sequences for training the model.

*Model Building:*

- A sequential model using LSTM layers is built to learn text patterns.

- Uses categorical crossentropy loss and an Adam optimizer.

*Training:*

- Model is trained on sequences to predict the next character/word.

- Weights are saved for reusability and faster inference.

*Text Generation:*

- A seed input is provided (e.g., "Once upon a time").

- The model predicts the next characters/words iteratively.

- Temperature sampling may be used to control randomness.


**How to Run**

Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt
Run the model:

bash
Copy
Edit
python text_generator.py
Provide a prompt when prompted or edit the text_generator.py to insert your own seed input.


**Learning Outcomes**
- Learned how to build LSTM-based language models.

- Understood sequence prediction and text tokenization.

- Gained hands-on experience with generating creative, context-aware text.

- Explored temperature-based sampling and its effect on creativity.


**Applications**
- Chatbots and virtual assistants

- Story or poetry generation

- Email autocomplete systems

- Language learning tools

- AI-based creative writing


**Sample Requirements (requirements.txt)**
- txt
- Copy
- Edit
- tensorflow
- numpy
- matplotlib


**Notes**
Longer training corpus = better results.

You can fine-tune a pre-trained GPT-like model (e.g., using Hugging Face Transformers) for advanced results.

Try using character-level vs word-level generation for different effects.


**Conclusion**
This task highlighted the creativity AI can bring into writing. By using LSTM-based models, we were able to teach the machine how to think in sequences, respond to prompts, and generate meaningful human-like content. It's a foundational step into advanced NLP domains like transformers and large language models.


**Output**





